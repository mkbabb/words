[tool:pytest]
# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Async configuration
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Output configuration
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --disable-warnings
    -ra
    --maxfail=10
    --cov=src/floridify
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-fail-under=80
    --benchmark-only
    --benchmark-sort=mean
    --benchmark-columns=min,max,mean,stddev,rounds,iterations

# Test markers  
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
    api: marks tests as API endpoint tests
    performance: marks tests as performance benchmarks
    benchmark: marks tests as benchmark tests (use --benchmark-only)
    ai: marks tests that use AI/OpenAI mocking
    database: marks tests that require database
    corpus: marks tests for corpus functionality
    wordlist: marks tests for wordlist functionality
    search: marks tests for search functionality
    lookup: marks tests for lookup pipeline

# Timeout configuration
timeout = 300
timeout_method = thread

# Filter warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore:.*unclosed.*:ResourceWarning
    ignore:.*coroutine.*was never awaited:RuntimeWarning