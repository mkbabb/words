[tool:pytest]
# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Async configuration
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Output configuration
addopts =
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --disable-warnings
    -ra
    --maxfail=10
    --cov=src/floridify
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-fail-under=80

# Test markers
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
    api: marks tests as API endpoint tests
    performance: marks tests as performance benchmarks
    benchmark: marks tests as benchmark tests (use --benchmark-only)
    ai: marks tests that use AI/OpenAI mocking
    database: marks tests that require database
    corpus: marks tests for corpus functionality
    wordlist: marks tests for wordlist functionality
    search: marks tests for search functionality
    lookup: marks tests for lookup pipeline
    semantic: marks tests that use semantic search (can be slow)
    timeout(seconds): specifies timeout for individual test

# Timeout configuration
# Global timeout for all tests (5 minutes)
timeout = 300
timeout_method = thread
# Individual test timeouts can be set with @pytest.mark.timeout(seconds)

# Filter warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore:.*unclosed.*:ResourceWarning
    ignore:.*coroutine.*was never awaited:RuntimeWarning
    ignore:.*numpy\.core\._multiarray_umath.*:DeprecationWarning
    ignore:.*builtin type.*has no __module__ attribute.*:DeprecationWarning
    ignore:.*pydantic\.generics.*:UserWarning