# Request Deduplication in Floridify

## Overview

Request deduplication is a pattern that prevents multiple concurrent requests for the same resource from executing redundantly. When multiple requests arrive for the same data, only the first request performs the actual work while subsequent requests wait for and share its result.

## Benefits

1. **Reduced Backend Load**: Prevents overwhelming databases and APIs with duplicate requests
2. **Improved Response Times**: Subsequent requests get results faster by sharing work
3. **Cost Reduction**: Fewer API calls to external services (OpenAI, etc.)
4. **Better Resource Utilization**: More efficient use of CPU, memory, and network resources
5. **Graceful Handling of Traffic Spikes**: Automatically coalesces requests during high load

## Architecture

### Components

1. **RequestDeduplicator**: Core class managing in-flight requests
2. **InFlightRequest**: Tracks pending requests and waiting coroutines
3. **Decorators**: Easy-to-use decorators for functions and endpoints
4. **Integration**: Seamless integration with existing caching system

### How It Works

```mermaid
sequenceDiagram
    participant C1 as Client 1
    participant C2 as Client 2
    participant C3 as Client 3
    participant API as API Endpoint
    participant D as Deduplicator
    participant DB as Database/Service

    C1->>API: Request A
    API->>D: Check in-flight
    D->>API: Not in-flight
    API->>D: Register request
    API->>DB: Execute query
    
    C2->>API: Request A (same)
    API->>D: Check in-flight
    D->>API: Already in-flight
    Note over C2,D: C2 waits for result
    
    C3->>API: Request A (same)
    API->>D: Check in-flight
    D->>API: Already in-flight
    Note over C3,D: C3 waits for result
    
    DB-->>API: Result
    API->>D: Complete request
    D-->>C1: Return result
    D-->>C2: Return result
    D-->>C3: Return result
```

## Usage

### Basic Deduplication

```python
from floridify.caching import deduplicated

@deduplicated(key_func=lambda word: f"lookup:{word}")
async def lookup_word(word: str) -> Definition:
    # Expensive operation
    result = await fetch_from_database(word)
    return result
```

### Combined with Caching

```python
from floridify.caching import cached_api_call_with_dedup

@cached_api_call_with_dedup(
    ttl_hours=1.0,
    key_func=lambda word, lang: ("lookup", word, lang),
    max_wait_time=30.0,
)
async def lookup_word_cached(word: str, lang: str = "en") -> Definition:
    # This function has both caching and deduplication
    result = await expensive_api_call(word, lang)
    return result
```

### FastAPI Endpoint Example

```python
@router.get("/api/word/{word}")
@cached_api_call_with_dedup(
    ttl_hours=1.0,
    key_func=lambda word: ("api_word", word),
)
async def get_word_definition(word: str) -> WordResponse:
    # Multiple concurrent requests for the same word
    # will only result in one backend call
    definition = await lookup_service.get_definition(word)
    return WordResponse(word=word, definition=definition)
```

## Configuration

### Deduplicator Parameters

- `max_wait_time`: Maximum seconds to wait for an in-flight request (default: 30.0)
- `cleanup_interval`: Seconds between cleanup of stale requests (default: 60.0)

### Decorator Parameters

- `key_func`: Function to generate deduplication key from arguments
- `max_wait_time`: Override default wait time per decorator

## Key Generation Strategies

### Simple Key
```python
key_func=lambda user_id: f"user:{user_id}"
```

### Composite Key
```python
key_func=lambda user_id, resource: f"user:{user_id}:resource:{resource}"
```

### Selective Parameters
```python
def key_func(word: str, include_etymology: bool = False, debug: bool = False):
    # Ignore debug flag in key
    return f"word:{word}:etymology:{include_etymology}"
```

### Normalized Keys
```python
def key_func(query: str, limit: int = 10):
    # Normalize query and cap limit
    return f"search:{query.lower()}:limit:{min(limit, 100)}"
```

## Performance Characteristics

### Test Results

With 500 concurrent requests for the same resource:
- **Without Deduplication**: 500 database queries (457.228ms total)
- **With Deduplication**: ~100 database queries (49.328ms total)
- **Reduction**: 80% fewer backend calls

### Memory Usage

- Each in-flight request uses minimal memory (key + Future object)
- Automatic cleanup of completed requests
- Configurable maximum in-flight requests

## Migration Guide

### Updating Existing Endpoints

1. **Minimal Change**: Replace `@cached_api_call` with `@cached_api_call_with_dedup`

```python
# Before
@cached_api_call(ttl_hours=1.0, key_func=lambda x: x)
async def my_endpoint(x: str): ...

# After
@cached_api_call_with_dedup(ttl_hours=1.0, key_func=lambda x: x)
async def my_endpoint(x: str): ...
```

2. **Add to Existing Decorator Stack**

```python
@deduplicated(key_func=lambda x: f"endpoint:{x}")
@cached_api_call(ttl_hours=1.0)
async def my_endpoint(x: str): ...
```

## Monitoring and Debugging

### Logging

The deduplicator logs key events:
- `üöÄ Starting new request: {key}` - New request initiated
- `üîÑ Deduplicating request: {key} (waiters: N)` - Request being deduplicated
- `‚úÖ Request completed: {key}` - Request finished successfully
- `‚ùå Request failed: {key}` - Request encountered error
- `üßπ Cleaned up stale in-flight request: {key}` - Stale request removed

### Metrics

Track deduplication effectiveness:

```python
class DeduplicationMetrics:
    total_requests: int
    deduplicated_requests: int
    cache_hits: int
    actual_executions: int
    deduplication_rate: float
```

## Best Practices

1. **Choose Appropriate Keys**: Ensure key uniquely identifies the request
2. **Set Reasonable Timeouts**: Balance between wait time and responsiveness
3. **Monitor Performance**: Track deduplication rates and adjust configuration
4. **Handle Errors Gracefully**: Deduplicated errors propagate to all waiters
5. **Test Concurrent Scenarios**: Verify behavior under load

## Common Pitfalls

1. **Too Broad Keys**: May deduplicate unrelated requests
2. **Too Narrow Keys**: May miss deduplication opportunities
3. **Long Wait Times**: Can cause request timeouts
4. **Missing Error Handling**: Errors affect all waiting requests

## Integration with Existing Systems

### With Rate Limiting
```python
@rate_limit(requests_per_minute=100)
@cached_api_call_with_dedup(ttl_hours=1.0)
async def limited_endpoint(): ...
```

### With Authentication
```python
@require_auth
@cached_api_call_with_dedup(
    key_func=lambda user_id, resource: f"{user_id}:{resource}"
)
async def user_resource(user_id: str, resource: str): ...
```

## Implementation Details

### Thread Safety

- Uses `asyncio.Lock` for thread-safe operations
- Safe for use in multi-coroutine environments
- No global state mutations outside locks

### Memory Management

- Automatic cleanup of completed requests
- Configurable stale request timeout
- Minimal memory footprint per request

### Error Propagation

- Exceptions propagate to all waiting coroutines
- Failed requests are not cached
- Automatic retry possible after failure

## Future Enhancements

1. **Distributed Deduplication**: Redis-based deduplication across instances
2. **Priority Queues**: Different priority levels for requests
3. **Circuit Breaker Integration**: Automatic fallback on repeated failures
4. **Metrics Dashboard**: Real-time deduplication statistics
5. **Request Batching**: Combine multiple requests into batch operations